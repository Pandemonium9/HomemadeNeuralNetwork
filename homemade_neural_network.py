# -*- coding: utf-8 -*-
"""Homemade Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pbjUyO2TKbL6wXJSDzdPGFuhS-vq_WgO
"""

import numpy as np

import tensorflow as tf#tensorflow only used for the dataset not for the neural network
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the images (28x28 to 784)
x_train = x_train.reshape(x_train.shape[0], 28 * 28)
x_test = x_test.reshape(x_test.shape[0], 28 * 28)


# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)


np.random.seed(42)# sets random seed to the universal answer for everything, for reproduceability
def clip_gradients(grad, clip_value=80):#clip_value decides what value it is clipped at, per my research, should be around 80
    return np.clip(grad, -clip_value, clip_value)# don't really need the -clip_value because we are using a reLU but just so it doesn't error
class Layer:# just for structure, functionally useless
  def __init__(self):
    self.input = None
    self.output = None
  def forward(self, input):#useless begins
    pass
  def backward(output_gradient,learning_rate): # just for structure
    pass#useless ends
class Dense(Layer):
  def __init__(self,input_size,output_size):
    self.weights = np.random.randn(input_size,output_size)# generate random values initially
    self.bias = np.random.randn(1,output_size)
  def forward(self,input):

    self.input = input
    return np.dot(input,self.weights) + self.bias# propagates values forward, follows z= X * w + b
    #z = output
    #X = input to the layer
    #w = weights
    #b = bias
  def backward(self,output_gradient,learning_rate):
    weights_gradient = np.dot(self.input.T, output_gradient)#Finds the derivative of the weights w/ respect to the loss function basically dot(input.T,loss)
    bias_gradient = np.sum(output_gradient, axis=0, keepdims=True)# derivative of bias with respect to the loss, keepdims keeps the dimensions of the input
    weights_gradient = clip_gradients(weights_gradient)# clipping the gradients to prevent exploding gradients
    bias_gradient = clip_gradients(bias_gradient)
    self.weights -= learning_rate * weights_gradient#propagates the values backward, the learning rate determines how much the change is
    self.bias -= learning_rate * bias_gradient
    return np.dot(output_gradient, self.weights.T)

class Activation(Layer):
  def forward(self,input):# just a reLU function
    self.input = input
    self.output = np.maximum(0,input)
    return self.output
  def backward(self,output_gradient,learning_rate):# the derivative of reLU return 0 if 0 or negative else just return the output_gradient
    return output_gradient * (self.input > 0)# it is complicated cus it has to work for vectors
class Loss(Layer):
  def __init__(self):
    pass
  def categorical_cross_entropy(self,y_true, y_pred):# got this off of the internet because its too much math
      epsilon = 1e-15 #prevents log(0)
      y_pred = np.clip(y_pred, epsilon, 1 - epsilon)# clips gradient to prevent explosion of gradients
      loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0] # computes categorical cross entropy, if you're not in calculus, don't worry about it
      return loss
  def loss_gradient(self,y_pred,y_true):
    return y_pred - y_true # the gradient of the loss function is just predicted - real, needs softmax to happen before tho
def softmax(logits):# more complicated math
  shifted_logits = logits - np.max(logits, axis=1, keepdims=True)
  exp_logits = np.exp(shifted_logits)
  # Compute softmax
  return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
model = [# make the layers of the model, this is the easy part
      Dense(28*28,128),
      Activation(),
      Dense(128,128),
      Activation(),
      Dense(128,10)
]


loss = Loss()
Epochs = 10#10 seems to be the best
Learning_rate = 0.0001#0.0001 seems good in my experimentation, but in more complex networks, it should be lower
def create_batches(X, y, batch_size):

    num_batches = len(X) // batch_size # gets number of batches

    for i in range(num_batches):

        start_idx = i * batch_size# finds the beginning of the batch

        end_idx = start_idx + batch_size# end of the batch

        yield X[start_idx:end_idx], y[start_idx:end_idx] # returning it
for epoch in range(Epochs):

  error = 0# makes the count for the error(how bad it is)
  for X_batch, y_batch in create_batches(x_train, y_train, batch_size=128):

    x=X_batch# simpler
    for layer in model: # goes through the full network
      x=layer.forward(x)
    output = softmax(x) #softmaxes the output of the last layer

    error += loss.categorical_cross_entropy(y_batch,output)# adds the loss for this batch to the running count

    grad = loss.loss_gradient(output,y_batch )#finds the gradient of the loss

    for layer in reversed(model):#goes through the network backward for backpass

      grad = layer.backward(grad , Learning_rate)#goes through the network backward computing the loss function for each layer and changeing weights and biases



  print("epoch:",epoch+1,"\nerror:",error)#prints out the error for the epoch

correct = 0
total = len(x_test)
for X_batch, y_batch in create_batches(x_test, y_test, batch_size=32):
    x = X_batch
    for layer in model:
        x = layer.forward(x)
    output = softmax(x)
    predictions = np.argmax(output, axis=1)

    correct += np.sum(predictions == np.argmax(y_batch, axis=1))


print(f"Test Accuracy: {(correct/total)*100}%")

digit = 50

some_digit = x_train[digit]
some_digit_image = some_digit.reshape(28,28)
plt.imshow(some_digit_image, cmap="binary")
plt.axis("off")
plt.show()
x= x_train[digit]
for layer in model:
  x=layer.forward(x)
x = softmax(x)
print(np.argmax(x))
